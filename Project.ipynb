{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001\n",
      "002\n",
      "004\n",
      "006\n",
      "001\n",
      "002\n",
      "003\n",
      "004\n",
      "005\n",
      "006\n",
      "001\n",
      "002\n",
      "003\n",
      "005\n",
      "001\n",
      "005\n",
      "006\n",
      "001\n",
      "002\n",
      "003\n",
      "006\n",
      "001\n",
      "001\n",
      "001\n",
      "003\n",
      "004\n",
      "005\n",
      "006\n",
      "001\n",
      "003\n",
      "005\n",
      "001\n",
      "005\n",
      "006\n",
      "001\n",
      "003\n",
      "006\n",
      "001\n",
      "002\n",
      "004\n",
      "006\n",
      "001\n",
      "003\n",
      "006\n",
      "004\n",
      "005\n",
      "001\n",
      "002\n",
      "003\n",
      "004\n",
      "001\n",
      "002\n",
      "004\n",
      "006\n",
      "002\n",
      "003\n",
      "001\n",
      "004\n",
      "006\n",
      "001\n",
      "004\n",
      "002\n",
      "003\n",
      "004\n",
      "001\n",
      "003\n",
      "004\n",
      "005\n",
      "006\n",
      "002\n",
      "003\n",
      "004\n",
      "001\n",
      "003\n",
      "006\n",
      "001\n",
      "005\n",
      "006\n",
      "001\n",
      "002\n",
      "002\n",
      "003\n",
      "005\n",
      "001\n",
      "002\n",
      "004\n",
      "001\n",
      "002\n",
      "004\n",
      "005\n",
      "001\n",
      "002\n",
      "001\n",
      "003\n",
      "004\n",
      "002\n",
      "003\n",
      "004\n",
      "005\n",
      "002\n",
      "003\n",
      "004\n",
      "005\n",
      "002\n",
      "004\n",
      "005\n",
      "006\n",
      "002\n",
      "003\n",
      "004\n",
      "005\n",
      "002\n",
      "003\n",
      "004\n",
      "002\n",
      "003\n",
      "005\n",
      "001\n",
      "002\n",
      "004\n",
      "005\n",
      "006\n",
      "005\n",
      "006\n",
      "001\n",
      "006\n",
      "001\n",
      "002\n",
      "004\n",
      "005\n",
      "002\n",
      "005\n",
      "006\n",
      "008\n",
      "001\n",
      "005\n",
      "006\n",
      "001\n",
      "006\n",
      "001\n",
      "004\n",
      "007\n",
      "001\n",
      "002\n",
      "004\n",
      "001\n",
      "005\n",
      "008\n",
      "001\n",
      "002\n",
      "008\n",
      "001\n",
      "005\n",
      "007\n",
      "003\n",
      "001\n",
      "002\n",
      "002\n",
      "003\n",
      "004\n",
      "001\n",
      "002\n",
      "001\n",
      "004\n",
      "005\n",
      "007\n",
      "001\n",
      "004\n",
      "001\n",
      "002\n",
      "003\n",
      "002\n",
      "006\n",
      "007\n",
      "001\n",
      "003\n",
      "001\n",
      "003\n",
      "004\n",
      "001\n",
      "004\n",
      "001\n",
      "004\n",
      "001\n",
      "006\n",
      "007\n",
      "010\n",
      "001\n",
      "003\n",
      "004\n",
      "001\n",
      "004\n",
      "006\n",
      "003\n",
      "004\n",
      "001\n",
      "004\n",
      "007\n",
      "002\n",
      "005\n",
      "006\n",
      "002\n",
      "002\n",
      "003\n",
      "009\n",
      "008\n",
      "002\n",
      "004\n",
      "006\n",
      "001\n",
      "005\n",
      "005\n",
      "006\n",
      "008\n",
      "003\n",
      "005\n",
      "006\n",
      "001\n",
      "001\n",
      "006\n",
      "007\n",
      "005\n",
      "001\n",
      "003\n",
      "008\n",
      "001\n",
      "006\n",
      "001\n",
      "004\n",
      "008\n",
      "001\n",
      "006\n",
      "007\n",
      "001\n",
      "003\n",
      "006\n",
      "001\n",
      "003\n",
      "008\n",
      "001\n",
      "001\n",
      "003\n",
      "006\n",
      "007\n",
      "001\n",
      "005\n",
      "006\n",
      "007\n",
      "008\n",
      "004\n",
      "008\n",
      "001\n",
      "004\n",
      "010\n",
      "004\n",
      "011\n",
      "002\n",
      "006\n",
      "011\n",
      "012\n",
      "001\n",
      "007\n",
      "009\n",
      "012\n",
      "013\n",
      "001\n",
      "003\n",
      "006\n",
      "010\n",
      "002\n",
      "003\n",
      "005\n",
      "006\n",
      "008\n",
      "003\n",
      "009\n",
      "010\n",
      "003\n",
      "004\n",
      "008\n",
      "001\n",
      "012\n",
      "001\n",
      "003\n",
      "005\n",
      "006\n",
      "001\n",
      "005\n",
      "011\n",
      "001\n",
      "004\n",
      "005\n",
      "007\n",
      "008\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "002\n",
      "006\n",
      "001\n",
      "004\n",
      "006\n",
      "001\n",
      "002\n",
      "004\n",
      "001\n",
      "002\n",
      "006\n",
      "001\n",
      "002\n",
      "004\n",
      "006\n",
      "002\n",
      "006\n",
      "001\n",
      "002\n",
      "004\n",
      "006\n",
      "002\n",
      "001\n",
      "003\n"
     ]
    }
   ],
   "source": [
    "#extract images.py\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] #Define emotion order\n",
    "participants = glob.glob(\"source_emotion\\\\*\") #Returns a list of all folders with participant numbers\n",
    "for x in participants:\n",
    "    part = \"%s\" %x[-4:] #store current participant number\n",
    "    for sessions in glob.glob(\"%s\\\\*\" %x):#Store list of sessions for current participant\n",
    "        for files in glob.glob(\"%s\\\\*\" %sessions):\n",
    "            #print(files)\n",
    "            current_session = files[20:-30] #[20:-30]\n",
    "            print(current_session)\n",
    "            file = open(files, 'r')\n",
    "            #print(file)\n",
    "            emotion = int(float(file.readline())) #emotions are encoded as a float, readline as float, then convert to integer.\n",
    "            #print(emotion)\n",
    "            sourcefile_emotion = glob.glob(\"source_images\\\\%s\\\\%s\\\\*\" %(part, current_session))[-1] #get path for last image in sequence, which contains the emotion\n",
    "            #print(sourcefile_emotion)\n",
    "            sourcefile_neutral = glob.glob(\"source_images\\\\%s\\\\%s\\\\*\" %(part, current_session))[0] #do same for neutral image\n",
    "            \n",
    "            dest_neut = \"sorted_set\\\\neutral\\\\%s\" %sourcefile_neutral[25:] #Generate path to put neutral image\n",
    "            dest_emot = \"sorted_set\\\\%s\\\\%s\" %(emotions[emotion], sourcefile_emotion[25:]) #Do same for emotion containing image\n",
    "            \n",
    "            copyfile(sourcefile_neutral, dest_neut) #Copy file\n",
    "            copyfile(sourcefile_emotion, dest_emot) #Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makesets.py\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "faceDet = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "faceDet_two = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "faceDet_three = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "faceDet_four = cv2.CascadeClassifier(\"haarcascade_frontalface_alt_tree.xml\")\n",
    "\n",
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] #Define emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9cf231bb0851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#for emotion in emotions:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m  \u001b[1;31m#   print('hi\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'surprise'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Call functiona\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-9cf231bb0851>\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(emotion)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Open image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Convert image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#Detect face using 4 different classifiers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "def detect_faces(emotion):\n",
    "    files = glob.glob(\"sorted_set\\\\%s\\\\*\" %emotion) #Get list of all images with emotion\n",
    "    #print(files)\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        frame = cv2.imread(f, 1) #Open image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        \n",
    "        #Detect face using 4 different classifiers\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "        face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "        face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "       \n",
    "        face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "\n",
    "        #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "        if len(face) == 1:\n",
    "            #print(face)\n",
    "            #print(len(face))\n",
    "            facefeatures = face\n",
    "        elif len(face_two) == 1:\n",
    "            #print(face_two)\n",
    "            facefeatures = face_two\n",
    "        elif len(face_three) == 1:\n",
    "            #print(face_three)\n",
    "            facefeatures = face_three\n",
    "        elif len(face_four) == 1:\n",
    "            #print(face_four)\n",
    "            facefeatures = face_four\n",
    "        else:\n",
    "            facefeatures = \"\"\n",
    "        \n",
    "        #Cut and save face\n",
    "        for (x, y, w, h) in facefeatures: #get coordinates and size of rectangle containing face\n",
    "            #print (\"face found in file: %s\" %f)\n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            \n",
    "            try:\n",
    "                out = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "                cv2.imwrite(\"dataset\\\\%s\\\\%s.jpg\" %(emotion, filenumber), out) #Write image\n",
    "                filenumber +=1\n",
    "                #print(filenumber)\n",
    "            except:\n",
    "               pass #If error, pass file\n",
    "               filenumber += 1 #Increment image number\n",
    "    print('Finish detect_faces')\n",
    "\n",
    "for emotion in emotions:\n",
    "    print('hi\\n')\n",
    "    detect_faces('surprise') #Call functiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-f044c3bb48a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mmetascore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_recognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"got\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"percent correct!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mmetascore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-f044c3bb48a6>\u001b[0m in \u001b[0;36mrun_recognizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_recognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-f044c3bb48a6>\u001b[0m in \u001b[0;36mmake_sets\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#open image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#convert to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#append image array to training data list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtraining_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] #Emotion list\n",
    "fishface = cv2.face.FisherFaceRecognizer_create() #Initialize fisher face classifier\n",
    "\n",
    "data = {}\n",
    "\n",
    "def get_files(emotion): #Define function to get file list, randomly shuffle it and split 80/20\n",
    "    files = glob.glob(\"dataset\\\\%s\\\\*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files)*0.8)] #get first 80% of file list\n",
    "    prediction = files[-int(len(files)*0.2):] #get last 20% of file list\n",
    "    return training, prediction\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for i in range(0,7):\n",
    "        emotion=emotions[i]\n",
    "        print(emotion)\n",
    "        training, prediction = get_files(emotion)\n",
    "        #Append data to training and prediction list, and generate labels 0-7\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) #open image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "            training_data.append(gray) #append image array to training data list\n",
    "            training_labels.append(emotions.index(emotion))\n",
    "    \n",
    "        for item in prediction: #repeat above process for prediction set\n",
    "            image = cv2.imread(item)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            print('lol')\n",
    "            prediction_data.append(gray)\n",
    "            print(prediction_data)\n",
    "            prediction_labels.append(emotions.index(emotion))\n",
    "        \n",
    "\n",
    "    return training_data, training_labels, prediction_data, prediction_labels\n",
    "\n",
    "\n",
    "def run_recognizer():\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "   \n",
    "    \n",
    "    print(\"training fisher face classifier\")\n",
    "    print(\"size of training lables is:\", len(np.asarray(training_labels)), \"images\")\n",
    "    print(\"size of training data is:\", len(training_data), \"images\")\n",
    "    fishface.train(training_data, np.asarray(training_labels))\n",
    "\n",
    "    print(\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in prediction_data:\n",
    "        pred, conf = fishface.predict(image)\n",
    "        if pred == prediction_labels[cnt]:\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    return ((100*correct)/(correct + incorrect))\n",
    "\n",
    "metascore = []\n",
    "for i in range(0,10):\n",
    "    correct = run_recognizer()\n",
    "    print(\"got\", correct, \"percent correct!\")\n",
    "    metascore.append(correct)\n",
    "\n",
    "print(\"\\n\\nend score:\", np.mean(metascore), \"percent correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_recognizer():\n",
    "    training_data, training_labels, prediction_data, prediction_labels = make_sets()\n",
    "   \n",
    "    \n",
    "    print(\"training fisher face classifier\")\n",
    "    print(\"size of training lables is:\", len(np.asarray(training_labels)), \"images\")\n",
    "    print(\"size of training data is:\", len(training_data), \"images\")\n",
    "    fishface.train(training_data, np.asarray(training_labels))\n",
    "\n",
    "    print(\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for image in prediction_data:\n",
    "        pred, conf = fishface.predict(image)\n",
    "        if pred == prediction_labels[cnt]:\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    return ((100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metascore = []\n",
    "for i in range(0,10):\n",
    "    correct = run_recognizer()\n",
    "    print(\"got\", correct, \"percent correct!\")\n",
    "    metascore.append(correct)\n",
    "\n",
    "print(\"\\n\\nend score:\", np.mean(metascore), \"percent correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking Image\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "facefeature=\"\";\n",
    "camera_port = 0\n",
    "ramp_frames = 30\n",
    "camera = cv2.VideoCapture(camera_port)\n",
    "def get_image():\n",
    "    \n",
    "    retval, im = camera.read()\n",
    "    return im\n",
    "\n",
    "temp = get_image()\n",
    "print(\"Taking Image\")\n",
    "camera_capture = get_image()\n",
    "#try:\n",
    "#    os.remove(\"capture/capture_image.png\")\n",
    "#    os.remove(\"capture/capture1.jpg\")\n",
    "\n",
    "#except:\n",
    "#    print(\"No files\")\n",
    "file = \"capture/capture_image.png\"\n",
    "cv2.imwrite(file, camera_capture)\n",
    "\n",
    "del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture\\capture_image.png\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-737dcb01cb53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mfilenumber\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-737dcb01cb53>\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceDet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mface2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceDet_two\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\imgproc\\src\\color.cpp:11147: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "def detect_faces():\n",
    "    global facefeature\n",
    "    files = glob.glob(\"capture/*\") \n",
    "    filenumber = 0\n",
    "    #print(files)\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        frame = cv2.imread(f, 0) \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face2 = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face3 = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face4 = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        if len(face) == 1:\n",
    "            facefeature = face\n",
    "        elif len(face2) == 1:\n",
    "            facefeature == face2\n",
    "        elif len(face3) == 1:\n",
    "            facefeature = face3\n",
    "        elif len(face4) == 1:\n",
    "            facefeature = face4\n",
    "        else:\n",
    "            facefeature = \"\"\n",
    "        print(facefeature)\n",
    "        for (x, y, w, h) in facefeature: \n",
    "            print (\"face found in file: %s\" %f)\n",
    "            gray = gray[y:y+h, x:x+w] \n",
    "            try:\n",
    "                out = cv2.resize(gray, (350, 350)) \n",
    "                cv2.imwrite(\"capture/capture1.jpg\" , out)\n",
    "                print(\"Face cropped and saved\\n\")\n",
    "            except:\n",
    "                pass \n",
    "                filenumber += 1\n",
    "detect_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting files from emotions neutral\n",
      "getting files from emotions anger\n",
      "getting files from emotions contempt\n",
      "getting files from emotions disgust\n",
      "getting files from emotions fear\n",
      "getting files from emotions happy\n",
      "getting files from emotions sadness\n",
      "getting files from emotions surprise\n",
      "training fisher face classifier\n",
      "size of training set is: 360 images\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\core\\src\\persistence_c.cpp:753: error: (-2) The node is neither a map nor an empty collection in function cvGetFileNodeByName\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-dc20735e5a99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pred = %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mrun_recognizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-181-dc20735e5a99>\u001b[0m in \u001b[0;36mrun_recognizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mfishfacetrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"predicting classification set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-181-dc20735e5a99>\u001b[0m in \u001b[0;36mfishfacetrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"size of training set is:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"images\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fishface.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mfishface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fishface.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mfishface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\bld\\opencv_1520732670222\\work\\opencv-3.4.1\\modules\\core\\src\\persistence_c.cpp:753: error: (-2) The node is neither a map nor an empty collection in function cvGetFileNodeByName\n"
     ]
    }
   ],
   "source": [
    "emotions = [ \"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"] \n",
    "fishface = cv2.face.FisherFaceRecognizer_create() \n",
    "\n",
    "data = {}\n",
    "def get_files(emotion): \n",
    "    print(\"getting files from emotions %s\" %emotion);\n",
    "    files = glob.glob(\"dataset/%s/*\" %emotion)\n",
    "    random.shuffle(files)\n",
    "    training = files[:int(len(files))]\n",
    "    #prediction = cv2.imread(\"capture/capture1.jpg\")\n",
    "    return training\n",
    "\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for emotion in emotions:\n",
    "        training = get_files(emotion)\n",
    "        for item in training:\n",
    "            image = cv2.imread(item) \n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "            training_data.append(gray) \n",
    "            training_labels.append(emotions.index(emotion))\n",
    "            #prediction_labels.append(emotions.index(emotion))\n",
    "    return training_data, training_labels\n",
    "\n",
    "def fishfacetrain():\n",
    "    training_data, training_labels = make_sets()\n",
    "    print (\"training fisher face classifier\")\n",
    "    print (\"size of training set is:\", len(training_labels), \"images\")\n",
    "    if(os.path.isfile(\"fishface.xml\")):\n",
    "        fishface.read(\"fishface.xml\")\n",
    "        return\n",
    "    fishface.train(training_data, np.asarray(training_labels))\n",
    "    fishface.save(\"fishface.xml\")\n",
    "\n",
    "\n",
    "\n",
    "def run_recognizer():\n",
    "    image = cv2.imread(\"capture/capture1.jpg\", 1)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('src',gray) \n",
    "    cv2.waitKey(1000)\n",
    "    cv2.destroyAllWindows()  \n",
    "    fishfacetrain();\n",
    "    print (\"predicting classification set\")\n",
    "    cnt = 0\n",
    "    correct = 0\n",
    "    incorrect = 0     \n",
    "    pred, conf = fishface.predict(gray)\n",
    "    print(\"pred = %s\" %(emotions[pred]))\n",
    "    return;\n",
    "run_recognizer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
